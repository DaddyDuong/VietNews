{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1439ee9a",
   "metadata": {},
   "source": [
    "# ZipVoice Vietnamese 2500h - Zero-Shot TTS on Google Colab\n",
    "\n",
    "**Zero-shot voice cloning for Vietnamese** | 123M parameters | 24kHz output | T4 GPU optimized\n",
    "\n",
    "ðŸ“„ [Paper](https://arxiv.org/abs/2506.13053) â€¢ ðŸ”— [GitHub](https://github.com/k2-fsa/ZipVoice) â€¢ ðŸŽµ [Demo](https://zipvoice.github.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4737bf58",
   "metadata": {},
   "source": [
    "## 1. Check GPU (T4 Required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c17d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4075aa",
   "metadata": {},
   "source": [
    "## 2. Setup: Clone Repositories & Download Reference Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1afdec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Clone repositories and download reference audio\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SETUP: Cloning repositories and downloading reference audio\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Clone ZipVoice repository from GitHub\n",
    "print(\"\\n[1/3] Cloning ZipVoice repository from GitHub...\")\n",
    "ZIPVOICE_REPO = \"/content/ZipVoice\"\n",
    "\n",
    "if os.path.exists(ZIPVOICE_REPO):\n",
    "    print(f\"âœ“ ZipVoice repository already exists at {ZIPVOICE_REPO}\")\n",
    "else:\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            \"git\", \"clone\", \n",
    "            \"https://github.com/k2-fsa/ZipVoice.git\",\n",
    "            ZIPVOICE_REPO\n",
    "        ], check=True, capture_output=True)\n",
    "        print(f\"âœ“ Successfully cloned ZipVoice repository to {ZIPVOICE_REPO}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"âœ— Failed to clone ZipVoice repository: {e}\")\n",
    "        raise\n",
    "\n",
    "# 2. Clone Vietnamese model from Hugging Face\n",
    "print(\"\\n[2/3] Cloning Vietnamese model from Hugging Face...\")\n",
    "MODEL_DIR = \"/content/ZipVoice-Vietnamese-2500h\"\n",
    "\n",
    "if os.path.exists(MODEL_DIR):\n",
    "    print(f\"âœ“ Model directory already exists at {MODEL_DIR}\")\n",
    "else:\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            \"git\", \"clone\",\n",
    "            \"https://huggingface.co/hynt/ZipVoice-Vietnamese-2500h\",\n",
    "            MODEL_DIR\n",
    "        ], check=True, capture_output=True)\n",
    "        print(f\"âœ“ Successfully cloned model to {MODEL_DIR}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"âœ— Failed to clone model repository: {e}\")\n",
    "        raise\n",
    "\n",
    "# 3. Download reference audio from Google Drive\n",
    "print(\"\\n[3/3] Downloading reference audio from Google Drive...\")\n",
    "REFERENCE_AUDIO = \"/content/reference_audio_3s.wav\"\n",
    "REFERENCE_TEXT = \"ChÃ o má»«ng quÃ½ vá»‹ vÃ  cÃ¡c báº¡n Ä‘Ã£ quay trá»Ÿ láº¡i vá»›i kÃªnh cá»§a chÃºng tÃ´i.\"\n",
    "\n",
    "# Google Drive direct download link for reference_audio_3s.wav\n",
    "# File ID from: https://drive.google.com/file/d/1PISVrfUSWRSj-zMrgmV-ABC5LTcLju-Z/view?usp=sharing\n",
    "AUDIO_FILE_ID = \"1PISVrfUSWRSj-zMrgmV-ABC5LTcLju-Z\"\n",
    "\n",
    "if os.path.exists(REFERENCE_AUDIO):\n",
    "    print(f\"âœ“ Reference audio already exists at {REFERENCE_AUDIO}\")\n",
    "else:\n",
    "    try:\n",
    "        # Install gdown if not already installed\n",
    "        try:\n",
    "            import gdown\n",
    "        except ImportError:\n",
    "            print(\"Installing gdown...\")\n",
    "            subprocess.run([\"pip\", \"install\", \"-q\", \"gdown\"], check=True)\n",
    "            import gdown\n",
    "        \n",
    "        # Download the reference audio file\n",
    "        url = f\"https://drive.google.com/uc?id={AUDIO_FILE_ID}\"\n",
    "        gdown.download(url, REFERENCE_AUDIO, quiet=False)\n",
    "        print(f\"âœ“ Successfully downloaded reference audio to {REFERENCE_AUDIO}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Could not download reference audio: {e}\")\n",
    "        print(\"You can manually upload reference_audio_3s.wav using the cell below.\")\n",
    "\n",
    "# Set global variables\n",
    "PROMPT_WAV = REFERENCE_AUDIO\n",
    "PROMPT_TEXT = REFERENCE_TEXT\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SETUP COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"âœ“ ZipVoice repository: {ZIPVOICE_REPO}\")\n",
    "print(f\"âœ“ Model directory: {MODEL_DIR}\")\n",
    "print(f\"âœ“ Reference audio: {PROMPT_WAV}\")\n",
    "print(f\"âœ“ Reference text: {PROMPT_TEXT}\")\n",
    "\n",
    "# Verify critical files\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nChecking model files...\")\n",
    "required_files = [\"config.json\", \"tokens.txt\", \"iter-525000-avg-2.pt\"]\n",
    "all_found = True\n",
    "for file in required_files:\n",
    "    file_path = f\"{MODEL_DIR}/{file}\"\n",
    "    if os.path.exists(file_path):\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(f\"âœ“ Found {file} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"âœ— Missing {file}\")\n",
    "        all_found = False\n",
    "\n",
    "if all_found:\n",
    "    print(\"\\nðŸŽ‰ All files ready! You can proceed to the next step.\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Some files are missing. Please check the repositories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99971fe5",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96889227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to ZipVoice directory and install dependencies\n",
    "%cd {ZIPVOICE_REPO}\n",
    "\n",
    "# Install required packages\n",
    "print(\"Installing ZipVoice dependencies...\")\n",
    "!pip install -q -r requirements.txt\n",
    "print(\"âœ“ Dependencies installed!\")\n",
    "\n",
    "# Install k2 for efficient inference\n",
    "print(\"\\nInstalling k2 for optimized inference...\")\n",
    "!pip install -q k2 -f https://k2-fsa.github.io/k2/cuda.html\n",
    "\n",
    "# Verify k2 installation\n",
    "try:\n",
    "    import k2\n",
    "    print(f\"âœ“ k2 installed successfully: {k2.__file__}\")\n",
    "except ImportError:\n",
    "    print(\"âš  k2 installation failed, but you can still run inference without it (slower)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0855917c",
   "metadata": {},
   "source": [
    "## 4. Load Model & Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d2bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and initialize all components\n",
    "import torch\n",
    "import json\n",
    "from zipvoice.models.zipvoice import ZipVoice\n",
    "from zipvoice.utils.checkpoint import load_checkpoint\n",
    "from zipvoice.tokenizer.tokenizer import EspeakTokenizer\n",
    "from zipvoice.utils.feature import VocosFbank\n",
    "from vocos import Vocos\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load configuration\n",
    "config_path = f\"{MODEL_DIR}/config.json\"\n",
    "print(f\"\\nLoading config from: {config_path}\")\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Count tokens\n",
    "tokens_path = f\"{MODEL_DIR}/tokens.txt\"\n",
    "with open(tokens_path, \"r\") as f:\n",
    "    num_tokens = len(f.readlines())\n",
    "\n",
    "print(f\"Model config loaded:\")\n",
    "print(f\"  - Feature dim: {config['model']['feat_dim']}\")\n",
    "print(f\"  - Sampling rate: {config['feature']['sampling_rate']} Hz\")\n",
    "print(f\"  - Number of tokens: {num_tokens}\")\n",
    "\n",
    "# Initialize model\n",
    "print(\"\\nInitializing ZipVoice model...\")\n",
    "model = ZipVoice(**config[\"model\"], vocab_size=num_tokens)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_path = f\"{MODEL_DIR}/iter-525000-avg-2.pt\"\n",
    "print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "load_checkpoint(checkpoint_path, model)\n",
    "model.to(device).eval()\n",
    "print(\"âœ“ Model loaded!\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "print(\"\\nInitializing tokenizer for Vietnamese...\")\n",
    "tokenizer = EspeakTokenizer(token_file=f\"{MODEL_DIR}/tokens.txt\", lang=\"vi\")\n",
    "print(\"âœ“ Tokenizer initialized!\")\n",
    "\n",
    "# Initialize feature extractor\n",
    "print(\"\\nInitializing feature extractor...\")\n",
    "feature_extractor = VocosFbank()\n",
    "print(\"âœ“ Feature extractor initialized!\")\n",
    "\n",
    "# Initialize vocoder\n",
    "print(\"\\nLoading Vocos vocoder...\")\n",
    "vocoder = Vocos.from_pretrained(\"charactr/vocos-mel-24khz\")\n",
    "vocoder.to(device).eval()\n",
    "print(\"âœ“ Vocoder loaded!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All components loaded successfully!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90c1af",
   "metadata": {},
   "source": [
    "## 6.1. Import Preprocessing Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a70f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import preprocessing utilities\n",
    "import datetime as dt\n",
    "import logging\n",
    "from zipvoice.utils.infer import (\n",
    "    add_punctuation, batchify_tokens, chunk_tokens_punctuation,\n",
    "    cross_fade_concat, load_prompt_wav, remove_silence, rms_norm,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Preprocessing utilities imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b1002c",
   "metadata": {},
   "source": [
    "## 6.2. Define Original Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c4bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(\n",
    "    save_path: str,\n",
    "    prompt_text: str,\n",
    "    prompt_wav: str,\n",
    "    text: str,\n",
    "    model,\n",
    "    vocoder,\n",
    "    tokenizer,\n",
    "    feature_extractor,\n",
    "    device,\n",
    "    num_step: int = 16,\n",
    "    guidance_scale: float = 1.0,\n",
    "    speed: float = 1.0,\n",
    "    t_shift: float = 0.5,\n",
    "    target_rms: float = 0.1,\n",
    "    feat_scale: float = 0.1,\n",
    "    sampling_rate: int = 24000,\n",
    "    max_duration: float = 100,\n",
    "    remove_long_sil: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate waveform of a text based on a given prompt waveform and its transcription.\n",
    "    This function includes full preprocessing pipeline from original ZipVoice repository:\n",
    "    1. Remove silences from prompt audio\n",
    "    2. Add punctuation if missing\n",
    "    3. Chunk text at punctuation marks\n",
    "    4. Process chunks in batches\n",
    "    5. Cross-fade audio chunks\n",
    "    6. Remove final silences\n",
    "    \n",
    "    Adapted from: ZipVoice/zipvoice/bin/infer_zipvoice.py\n",
    "    \"\"\"\n",
    "    \n",
    "    # Suppress torchaudio deprecation warnings\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore', category=UserWarning, module='torchaudio')\n",
    "    \n",
    "    # Step 1: Load and clean prompt audio\n",
    "    prompt_wav_tensor = load_prompt_wav(prompt_wav, sampling_rate=sampling_rate)\n",
    "    \n",
    "    # Remove edge and long silences in the prompt wav.\n",
    "    # Add 0.2s trailing silence to avoid leaking prompt to generated speech.\n",
    "    prompt_wav_tensor = remove_silence(\n",
    "        prompt_wav_tensor, sampling_rate, only_edge=False, trail_sil=200\n",
    "    )\n",
    "    \n",
    "    prompt_wav_tensor, prompt_rms = rms_norm(prompt_wav_tensor, target_rms)\n",
    "    prompt_duration = prompt_wav_tensor.shape[-1] / sampling_rate\n",
    "    \n",
    "    if prompt_duration > 20:\n",
    "        logging.warning(\n",
    "            f\"Given prompt wav is too long ({prompt_duration}s). \"\n",
    "            f\"Please provide a shorter one (1-3 seconds is recommended).\"\n",
    "        )\n",
    "    elif prompt_duration > 10:\n",
    "        logging.warning(\n",
    "            f\"Given prompt wav is long ({prompt_duration}s). \"\n",
    "            f\"It will lead to slower inference speed and possibly worse speech quality.\"\n",
    "        )\n",
    "    \n",
    "    # Step 2: Extract features from cleaned prompt\n",
    "    prompt_features = feature_extractor.extract(\n",
    "        prompt_wav_tensor, sampling_rate=sampling_rate\n",
    "    ).to(device)\n",
    "    prompt_features = prompt_features.unsqueeze(0) * feat_scale\n",
    "    \n",
    "    # Step 3: Add punctuation if missing\n",
    "    text = add_punctuation(text)\n",
    "    prompt_text = add_punctuation(prompt_text)\n",
    "    \n",
    "    # Step 4: Tokenize text (string tokens), punctuations preserved\n",
    "    tokens_str = tokenizer.texts_to_tokens([text])[0]\n",
    "    prompt_tokens_str = tokenizer.texts_to_tokens([prompt_text])[0]\n",
    "    \n",
    "    # Step 5: Chunk text at punctuation marks\n",
    "    # Calculate token duration from prompt\n",
    "    token_duration = (prompt_wav_tensor.shape[-1] / sampling_rate) / (\n",
    "        len(prompt_tokens_str) * speed\n",
    "    )\n",
    "    # Each chunk should be ~25 seconds\n",
    "    max_tokens = int((25 - prompt_duration) / token_duration)\n",
    "    chunked_tokens_str = chunk_tokens_punctuation(tokens_str, max_tokens=max_tokens)\n",
    "    \n",
    "    # Step 6: Convert string tokens to IDs\n",
    "    chunked_tokens = tokenizer.tokens_to_token_ids(chunked_tokens_str)\n",
    "    prompt_tokens = tokenizer.tokens_to_token_ids([prompt_tokens_str])\n",
    "    \n",
    "    # Step 7: Batchify chunks for efficient processing\n",
    "    tokens_batches, chunked_index = batchify_tokens(\n",
    "        chunked_tokens, max_duration, prompt_duration, token_duration\n",
    "    )\n",
    "    \n",
    "    # Step 8: Generate features for each batch\n",
    "    chunked_features = []\n",
    "    start_t = dt.datetime.now()\n",
    "    \n",
    "    for batch_tokens in tokens_batches:\n",
    "        batch_prompt_tokens = prompt_tokens * len(batch_tokens)\n",
    "        batch_prompt_features = prompt_features.repeat(len(batch_tokens), 1, 1)\n",
    "        batch_prompt_features_lens = torch.full(\n",
    "            (len(batch_tokens),), prompt_features.size(1), device=device\n",
    "        )\n",
    "        \n",
    "        # Generate features\n",
    "        (\n",
    "            pred_features,\n",
    "            pred_features_lens,\n",
    "            pred_prompt_features,\n",
    "            pred_prompt_features_lens,\n",
    "        ) = model.sample(\n",
    "            tokens=batch_tokens,\n",
    "            prompt_tokens=batch_prompt_tokens,\n",
    "            prompt_features=batch_prompt_features,\n",
    "            prompt_features_lens=batch_prompt_features_lens,\n",
    "            speed=speed,\n",
    "            t_shift=t_shift,\n",
    "            duration=\"predict\",\n",
    "            num_step=num_step,\n",
    "            guidance_scale=guidance_scale,\n",
    "        )\n",
    "        \n",
    "        # Postprocess predicted features\n",
    "        pred_features = pred_features.permute(0, 2, 1) / feat_scale  # (B, C, T)\n",
    "        chunked_features.append((pred_features, pred_features_lens))\n",
    "    \n",
    "    # Step 9: Convert features to waveforms with vocoder\n",
    "    chunked_wavs = []\n",
    "    start_vocoder_t = dt.datetime.now()\n",
    "    \n",
    "    for pred_features, pred_features_lens in chunked_features:\n",
    "        batch_wav = []\n",
    "        for i in range(pred_features.size(0)):\n",
    "            wav = (\n",
    "                vocoder.decode(pred_features[i][None, :, : pred_features_lens[i]])\n",
    "                .squeeze(1)\n",
    "                .clamp(-1, 1)\n",
    "            )\n",
    "            # Adjust wav volume if necessary\n",
    "            if prompt_rms < target_rms:\n",
    "                wav = wav * prompt_rms / target_rms\n",
    "            batch_wav.append(wav)\n",
    "        chunked_wavs.extend(batch_wav)\n",
    "    \n",
    "    # Finish model generation timing\n",
    "    t = (dt.datetime.now() - start_t).total_seconds()\n",
    "    \n",
    "    # Step 10: Merge chunked wavs with cross-fading\n",
    "    indexed_chunked_wavs = [\n",
    "        (index, wav) for index, wav in zip(chunked_index, chunked_wavs)\n",
    "    ]\n",
    "    sequential_indexed_chunked_wavs = sorted(indexed_chunked_wavs, key=lambda x: x[0])\n",
    "    sequential_chunked_wavs = [\n",
    "        sequential_indexed_chunked_wavs[i][1]\n",
    "        for i in range(len(sequential_indexed_chunked_wavs))\n",
    "    ]\n",
    "    final_wav = cross_fade_concat(\n",
    "        sequential_chunked_wavs, fade_duration=0.1, sample_rate=sampling_rate\n",
    "    )\n",
    "    \n",
    "    # Step 11: Remove final silences\n",
    "    final_wav = remove_silence(\n",
    "        final_wav, sampling_rate, only_edge=(not remove_long_sil), trail_sil=0\n",
    "    )\n",
    "    \n",
    "    # Calculate processing time metrics\n",
    "    t_no_vocoder = (start_vocoder_t - start_t).total_seconds()\n",
    "    t_vocoder = (dt.datetime.now() - start_vocoder_t).total_seconds()\n",
    "    wav_seconds = final_wav.shape[-1] / sampling_rate\n",
    "    rtf = t / wav_seconds\n",
    "    rtf_no_vocoder = t_no_vocoder / wav_seconds\n",
    "    rtf_vocoder = t_vocoder / wav_seconds\n",
    "    \n",
    "    metrics = {\n",
    "        \"t\": t,\n",
    "        \"t_no_vocoder\": t_no_vocoder,\n",
    "        \"t_vocoder\": t_vocoder,\n",
    "        \"wav_seconds\": wav_seconds,\n",
    "        \"rtf\": rtf,\n",
    "        \"rtf_no_vocoder\": rtf_no_vocoder,\n",
    "        \"rtf_vocoder\": rtf_vocoder,\n",
    "        \"num_chunks\": len(chunked_tokens),\n",
    "        \"prompt_duration\": prompt_duration,\n",
    "    }\n",
    "    \n",
    "    # Save the generated audio\n",
    "    import torchaudio\n",
    "    torchaudio.save(save_path, final_wav.cpu(), sample_rate=sampling_rate)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"âœ“ generate_sentence() function defined successfully!\")\n",
    "print(\"\\nThis function includes full preprocessing pipeline:\")\n",
    "print(\"  1. Prompt audio cleaning (silence removal)\")\n",
    "print(\"  2. Auto-punctuation addition\")\n",
    "print(\"  3. Text chunking at punctuation marks\")\n",
    "print(\"  4. Batch processing for efficiency\")\n",
    "print(\"  5. Cross-fading between audio chunks\")\n",
    "print(\"  6. Final silence removal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5339d538",
   "metadata": {},
   "source": [
    "## 6.3. Generate with Original Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712a319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Vietnamese speech\n",
    "import torch\n",
    "\n",
    "# === TEXT INPUT FOR AUTOMATION ===\n",
    "TEXT_TO_SYNTHESIZE = \"Xin chÃ o, tÃ´i lÃ  trá»£ lÃ½ áº£o cá»§a báº¡n. TÃ´i cÃ³ thá»ƒ giÃºp gÃ¬ cho báº¡n hÃ´m nay?\"\n",
    "# === END TEXT INPUT ===\n",
    "\n",
    "OUTPUT_WAV = \"/content/output_vietnamese.wav\"\n",
    "\n",
    "# Parameters\n",
    "NUM_STEP = 8\n",
    "SPEED = 1.0\n",
    "REMOVE_LONG_SIL = False\n",
    "MAX_DURATION = 100\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GENERATING SPEECH\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Text: {TEXT_TO_SYNTHESIZE[:60]}...\")\n",
    "print(f\"Output: {OUTPUT_WAV}\")\n",
    "print(f\"Params: num_step={NUM_STEP}, speed={SPEED}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"âœ“ GENERATION_STARTED\")\n",
    "\n",
    "# Generate\n",
    "with torch.inference_mode():\n",
    "    metrics = generate_sentence(\n",
    "        save_path=OUTPUT_WAV,\n",
    "        prompt_text=PROMPT_TEXT,\n",
    "        prompt_wav=PROMPT_WAV,\n",
    "        text=TEXT_TO_SYNTHESIZE,\n",
    "        model=model,\n",
    "        vocoder=vocoder,\n",
    "        tokenizer=tokenizer,\n",
    "        feature_extractor=feature_extractor,\n",
    "        device=device,\n",
    "        num_step=NUM_STEP,\n",
    "        guidance_scale=1.0,\n",
    "        speed=SPEED,\n",
    "        t_shift=0.5,\n",
    "        target_rms=0.1,\n",
    "        feat_scale=0.1,\n",
    "        sampling_rate=config[\"feature\"][\"sampling_rate\"],\n",
    "        max_duration=MAX_DURATION,\n",
    "        remove_long_sil=REMOVE_LONG_SIL,\n",
    "    )\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"âœ… COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Duration: {metrics['wav_seconds']:.2f}s | Time: {metrics['t']:.2f}s | RTF: {metrics['rtf']:.3f}x\")\n",
    "print(f\"Chunks: {metrics['num_chunks']} | Saved: {OUTPUT_WAV}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"âœ“ GENERATION_COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaa6986",
   "metadata": {},
   "source": [
    "## 7. Play Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75491b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the generated audio\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "print(\"Reference:\")\n",
    "display(Audio(PROMPT_WAV))\n",
    "\n",
    "print(\"\\nGenerated:\")\n",
    "display(Audio(OUTPUT_WAV))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db6a92",
   "metadata": {},
   "source": [
    "## 8. Download Generated Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c38dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the generated audio file\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Downloading generated audio...\")\n",
    "files.download(OUTPUT_WAV)\n",
    "print(f\"âœ“ Download complete: {OUTPUT_WAV}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
